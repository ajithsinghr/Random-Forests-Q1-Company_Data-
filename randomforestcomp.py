# -*- coding: utf-8 -*-
"""Randomforestcomp.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18Dz_GjbyvnzPiAv-fl5oyUm5yOeZJR91
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier

df =pd.read_csv("Company_Data.csv")
df.head()
df.describe()

# Converting Target variable 'Sales' into categories Low, Medium and High.
df['Sales'] = pd.cut(x=df['Sales'],bins=[0, 6, 12, 17], labels=['Low','Medium', 'High'], right = False)
df['Sales']

#visualization
import seaborn as sns
import matplotlib.pyplot as plt

# Check Correlation amoung parameters
corr = df.corr()
fig, ax = plt.subplots(figsize=(8,8))
# Generate a heatmap
sns.heatmap(corr, cmap = 'magma', annot = True, fmt = ".2f")
plt.xticks(range(len(corr.columns)), corr.columns)

plt.yticks(range(len(corr.columns)), corr.columns)

plt.show()

#pairwise plot of all the features
sns.pairplot(df)
plt.show()

# checking count of categories for categorical columns colums
sns.countplot(df['ShelveLoc'])
plt.show()

sns.countplot(df['Urban'])
plt.show()

sns.countplot(df['US'])
plt.show()

sns.countplot(df['Sales'])
plt.show()

#Label Encoder
from sklearn.preprocessing import LabelEncoder
LE =LabelEncoder()
df.iloc[:,6] = LE.fit_transform(df.iloc[:,6])
df.iloc[:,9] = LE.fit_transform(df.iloc[:,9])
df.iloc[:,10] = LE.fit_transform(df.iloc[:,10])
df.head()

#splitting the data into x and y
x = df.drop('Sales',axis=1)
y = df["Sales"]

#Data Partition
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.33,random_state=(40))

#Random forest
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import KFold
kfold=KFold()

num_trees = 100
max_features = 'auto'
skfolds=StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)

# Train the model on training data
model.fit(x_train, y_train)
y_pred_test = model.predict(x_test)

from sklearn.metrics import accuracy_score
ac = accuracy_score(y_test,y_pred_test)
print("test score:",ac.round(2))

results = cross_val_score(model, x_train, y_train, cv=kfold)
print(results.mean())

#Bagging
from sklearn.ensemble import BaggingClassifier

skfolds=StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
cart = DecisionTreeClassifier()
num_trees = 100
bag = BaggingClassifier(base_estimator=cart, n_estimators=num_trees, random_state=42)

# Train the model on training data
bag.fit(x_train, y_train)
y_pred_test = bag.predict(x_test)

ac = accuracy_score(y_test,y_pred_test)
print("test score:",ac.round(2))

results = cross_val_score(bag, x_train, y_train, cv=kfold)
print(results.mean())

#Ada boost
from sklearn.ensemble import AdaBoostClassifier

skfolds=StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
ada = AdaBoostClassifier(n_estimators=10, random_state=42)

ada.fit(x_train, y_train)
y_pred_test = ada.predict(x_test)

ac = accuracy_score(y_test,y_pred_test)
print("test score:",ac.round(2))

results = cross_val_score(ada, x_train, y_train, cv=kfold)
print(results.mean())

